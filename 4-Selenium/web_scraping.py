from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time, json

#  ç¬¬ä¸€éšæ®µï¼šæŠ“ PChome æœå°‹é æ‰€æœ‰å•†å“è³‡æ–™ï¼ˆå«å¯èƒ½ç‚º null çš„åœ–ç‰‡ï¼‰

def crawl_search_list(keyword):

    print("\n=====ã€éšæ®µ 1ï¼šæŠ“æœå°‹é æ‰€æœ‰å•†å“ã€‘=====\n")

    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")
    driver = webdriver.Chrome(options=options)

    results = []

    for page in range(1, 200):
        print(f"=== æŠ“ç¬¬ {page} é  ===")

        url = f"https://24h.pchome.com.tw/search/?q={keyword}&p={page}"
        driver.get(url)

        # ç­‰å¾…å•†å“è¼‰å…¥
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((
                    By.CSS_SELECTOR,
                    ".c-prodInfoV2__title, .c-product__name"
                ))
            )
        except:
            print("é é¢ç„¡å•†å“ â†’ åœæ­¢")
            break

        time.sleep(1)

        # æŠ“å…©ç¨® layout
        cards1 = driver.find_elements(By.CSS_SELECTOR, ".c-prodInfoV2")
        cards2 = driver.find_elements(By.CSS_SELECTOR, ".c-product")
        cards = cards1 + cards2

        print("æœ¬é å•†å“æ•¸ï¼š", len(cards))
        is_last_page = len(cards) < 40  

        # å°‡æ¯å€‹å¡ç‰‡çš„åŸºæœ¬è³‡æ–™æŠ“èµ·ä¾†
        for card in cards:

            # å¼·åˆ¶æ»¾å‹•ç¢ºä¿è³‡æ–™é¡¯ç¤º
            driver.execute_script("arguments[0].scrollIntoView();", card)
            time.sleep(0.15)

            # åç¨±
            try:
                name = card.find_element(
                    By.CSS_SELECTOR,
                    ".c-prodInfoV2__title, .c-product__name"
                ).text.strip()
            except:
                name = ""

            # åƒ¹æ ¼
            try:
                price = card.find_element(
                    By.CSS_SELECTOR,
                    ".c-prodInfoV2__priceValue--m, .c-product__price"
                ).text.strip()
            except:
                price = ""

            # é€£çµ
            try:
                link = card.find_element(
                    By.CSS_SELECTOR,
                    "a.c-prodInfoV2__link, a.c-product__img, a"
                ).get_attribute("href")
            except:
                link = ""

            # åœ–ç‰‡ï¼ˆåˆ—è¡¨é å…ˆæŠ“ï¼Œæœ‰å¯èƒ½ç‚º nullï¼‰
            img = None
            try:
                container = card.find_element(
                    By.CSS_SELECTOR,
                    ".c-prodInfoV2__img, .c-product__img"
                )

                # <img> æ¨™ç±¤
                try:
                    img_tag = container.find_element(By.TAG_NAME, "img")
                    img = img_tag.get_attribute("src")
                    if img and "loading.svg" in img:
                        img = img_tag.get_attribute("data-src")
                except:
                    pass

                # èƒŒæ™¯åœ–ç‰‡
                if not img:
                    style = container.get_attribute("style")
                    if "background-image" in style:
                        img = style.split("url(")[1].split(")")[0].strip('"').strip("'")

            except:
                img = None

            results.append({
                "name": name,
                "price": price,
                "link": link,
                "img": img
            })

        if is_last_page:
            print("â†’ å·²åˆ°æœ€å¾Œä¸€é ï¼Œåœæ­¢")
            break

        time.sleep(1)

    driver.quit()
    print(f"\nâœ” éšæ®µ 1 å®Œæˆï¼Œå…± {len(results)} ç­†å•†å“\n")
    return results

#  ç¬¬äºŒéšæ®µï¼šè£œä¸Šæ‰€æœ‰ null åœ–ç‰‡ï¼ˆé€²å…¥å•†å“é æŠ“å°é¢ï¼‰

def fill_cover_images(results):

    print("\n=====ã€éšæ®µ 2ï¼šè£œåœ–ç‰‡ï¼ˆæŠ“å•†å“é å°é¢ï¼‰ã€‘=====\n")

    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")
    driver = webdriver.Chrome(options=options)

    for item in results:

        if item["img"]:   # è‹¥åˆ—è¡¨é å·²æœ‰åœ–ç‰‡ â†’ è·³é
            continue

        link = item["link"]
        if not link:
            continue

        print(f"â†’ åˆ—è¡¨ç„¡åœ–ç‰‡ï¼Œé€²å…¥å•†å“é è£œæŠ“ï¼š{link}")

        try:
            driver.get(link)

            WebDriverWait(driver, 8).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".pic-main img"))
            )
            time.sleep(0.4)

            cover = driver.find_element(By.CSS_SELECTOR, ".pic-main img").get_attribute("src")
            item["img"] = cover

        except:
            print("   âŒ è£œæŠ“å¤±æ•—")
            item["img"] = None

    driver.quit()
    print("\nâœ” éšæ®µ 2 å®Œæˆï¼ˆåœ–ç‰‡è£œæŠ“çµæŸï¼‰\n")
    return results

def search_pchome_final(keyword):

    data = crawl_search_list(keyword)
    data = fill_cover_images(data)

    filename = f"{keyword}_final.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ã€å…¨éƒ¨å®Œæˆï¼ã€‘")
    print(f"âœ” æœ€çµ‚ç­†æ•¸ï¼š{len(data)}")
    print(f"âœ” å·²è¼¸å‡ºï¼š{filename}")

    return data

search_pchome_final("è¡›ç”Ÿç´™")
